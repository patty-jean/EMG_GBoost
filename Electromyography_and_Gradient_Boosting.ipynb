{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Electromyography and Gradient Boosting \n",
        "\n",
        "In this assignment I applied the Gradient Boosting ensemble prediction method I learned about in class with sklearn methods."
      ],
      "metadata": {
        "id": "dKsF6d4zwxfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background and Documentation\n",
        "\n",
        "Make sure you go through [this](https://explained.ai/gradient-boosting/),  [this](https://www.kaggle.com/code/kashnitsky/topic-10-gradient-boosting/notebook) and [this](https://www.gormanalysis.com/blog/gradient-boosting-explained/) excellent writeups. Summarise the technique including the equations in a markdown file or notebook. "
      ],
      "metadata": {
        "id": "7qgtfWw4wz-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Technique Summary:**\n",
        "\n",
        "The key of gradient boosting is that it is a model that utilizes weak learners, that while functional, tend to not be very accurate, and uses them additively to create a much more accurate model.  It is a non parametric method.  It does not look back at past inaccurate iterations of the model and changes them.  Instead, gradient boosting uses previous models to add to the success of future iterations of the model.  Any kind of weak learner can be used for gradient boosting, but trees are used very often, and are used in the implementation below with sklearn.  So let's say we have a weak learner which is a decision tree $g(x,\\theta)$\n",
        "\n",
        "To create a model that is as accurate as it can be, the goal is to minimize loss.  So a loss function that is differentiable must be used.  The purpose of this is that the negative gradient of the loss function will point in the direction of the minimum loss.  With gradient boosting, we want to cultivate a tree that will accurately model the negative gradient of the loss function to find the least loss.\n",
        "\n",
        "The idea of gradient boosting is the following:\n",
        "\n",
        "1. Initialize regression tree $g(x)$\n",
        "2. For $j = 1$ to the number of boost stages $M$:\n",
        "  * Calculate the residual, which is the negative gradient of the loss function $r_{ij}=-\\frac{\\partial L(y_i,\\hat{y}_i^{j-1})}{\\partial \\hat{y}_i^{j-1}}$ where $\\hat{y}_i$ is the predicted $y$ value and $y_i$ is the actual $y$ value.\n",
        "  * Fit our predictor $g(x)$ to the residual $r_{ij}$ to get $\\hat{y}^{j-1}$.  (We want to do this because as mentioned earlier, our negative gradient will point towards the least loss, which is our most accurate solution.)\n",
        "  * Update our predictor $g(x)$ with: $g_j(x)=g_{j-1}(x)+n\\hat{y}^{j-1}$ where n is our learning rate.\n",
        "\n",
        "3. Return the final boosted regression tree $g_M(x)$"
      ],
      "metadata": {
        "id": "eykZaNBa5z9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application on Cosinusoidal Dataset\n",
        "\n",
        "Below, I applied sklearn gradient boosting using the clipped cosinusoidal dataset for [the classification task](https://www.kaggle.com/code/kashnitsky/topic-10-gradient-boosting?scriptVersionId=37852307&cellId=15) mentioned here.\n",
        "![](https://pantelis.github.io/data-science/_images/clipped-cos-dataset.png)\n",
        "\n",
        "Cosinusoidal dataset:\n",
        "$\\large y = cos(x) + \\epsilon, \\epsilon \\sim \\mathcal{N}(0, \\frac{1}{5}), x \\in [-5,5]$"
      ],
      "metadata": {
        "id": "rZVHw5taw3J0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "###\n",
        "# First generate cosinusoidal dataset\n",
        "###\n",
        "\n",
        "x = []\n",
        "x_val = np.arange(-5,5,10.0/300.0)\n",
        "\n",
        "\n",
        "for _ in range(300):\n",
        "  x.append(-5 + random.random()*10)\n",
        "\n",
        "x = np.array(x)\n",
        "\n",
        "y = []\n",
        "y_val = []\n",
        "\n",
        "for i in range(len(x)):\n",
        "  err = random.gauss(0, 1.0/5.0)\n",
        "  cosi = math.cos(x[i])+err\n",
        "  y.append(cosi)\n",
        "  y_val.append(math.cos(x_val[i]))\n",
        "\n",
        "y = np.sign(np.array(y))\n",
        "y_val = np.sign(np.array(y_val))\n",
        "\n",
        "\n",
        "###\n",
        "# Plot the dataset. For fun :-)\n",
        "###\n",
        "plt.plot(x,y, '.')\n",
        "plt.plot(x_val, y_val)\n",
        "plt.title(\"Sign of Consinusoidal Data Over [-5,5] Overlayed Over Noisy Sign of Cosinusoidal Data\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "###\n",
        "# Use sklearn gradient boost!\n",
        "###\n",
        "x = x.reshape(-1,1)\n",
        "x_val = x_val.reshape(-1,1)\n",
        "\n",
        "learning_rate = 0.1\n",
        "gb = GradientBoostingClassifier(n_estimators=3, learning_rate=learning_rate, max_depth=2)\n",
        "gb.fit(x, y)\n",
        "\n",
        "print(\"Learning rate: \", learning_rate)\n",
        "print(\"Accuracy score with training data: {0:.3f}\".format(gb.score(x, y)))\n",
        "print(\"Accuracy score with testing data: {0:.3f}\".format(gb.score(x_val, y_val)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "Jdj2EBwlWcPJ",
        "outputId": "b298548e-2f5a-42ee-f939-44b2fb841b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate:  0.1\n",
            "Accuracy score with training data: 0.937\n",
            "Accuracy score with testing data: 0.960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EMG Dataset\n",
        "\n",
        "\n",
        "Below I applied gradient boosting implementation to the [EMG dataset](https://archive.ics.uci.edu/ml/datasets/EMG+Physical+Action+Data+Set) shown here to classify either aggressive (1) or non aggressive (0) actions.  I also tried out sets of different learning rates and n_estimators to find which one would create the best accuracy."
      ],
      "metadata": {
        "id": "WAkAgO1IxHOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive so that I can get the data from it\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uGRqXW8e4fM",
        "outputId": "e45e9a1e-2c84-458f-c78b-c9fc24554663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "\n",
        "###\n",
        "# Import dataset.  I uploaded it to drive and imported it from there, so I'm\n",
        "# unsure how it'll work when used by someone else.\n",
        "###\n",
        "path = '/content/drive/My Drive/dataset'\n",
        "%cd '/content/drive/My Drive/dataset'\n",
        "\n",
        "# Set aside which files have aggressive[1] actions and which have nonaggressive[0] actions.\n",
        "aggro = ['Slapping.txt', 'Sidekicking.txt', 'Pushing.txt', 'Punching.txt', 'Pulling.txt', 'Kneeing.txt', 'Headering.txt', 'Hamering.txt', 'Frontkicking.txt', 'Elbowing.txt']\n",
        "non = ['Waving.txt', 'Walking.txt', 'Running.txt', 'Standing.txt', 'Jumping.txt', 'Hugging.txt', 'Handshaking.txt', 'Seating.txt', 'Bowing.txt', 'Clapping.txt']\n",
        "\n",
        "aggroDat = np.loadtxt(aggro[0])\n",
        "nonDat = np.loadtxt(non[0])\n",
        "\n",
        "# Combine the data into two separate arrays, one for the aggressive actions, one for the nonaggressive actions\n",
        "for i in range(len(aggro)):\n",
        "  if i == 0:\n",
        "    continue\n",
        "  aDat = np.loadtxt(aggro[i])\n",
        "  nDat = np.loadtxt(non[i])\n",
        "  aggroDat = np.concatenate((aggroDat, aDat))\n",
        "  nonDat = np.concatenate((nonDat, nDat))\n",
        "\n",
        "%cd \"/content/\"\n",
        "\n",
        "ay = [1]*aggroDat.shape[0]\n",
        "ny = [0]*nonDat.shape[0]\n",
        "\n",
        "# Split into training and test data\n",
        "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(aggroDat, ay, test_size = 0.2, random_state = 4)\n",
        "X_train_n, X_test_n, y_train_n, y_test_n = train_test_split(nonDat, ny, test_size = 0.2, random_state = 4)\n",
        "\n",
        "\n",
        "# Combine the 0 and 1 training and testing sets into one training and one testing set\n",
        "X_train = np.vstack((X_train_a,X_train_n))\n",
        "X_test = np.vstack((X_test_a,X_test_n))\n",
        "y_train = np.hstack((y_train_a,y_train_n))\n",
        "y_test = np.hstack((y_test_a,y_test_n))\n",
        "\n",
        "\n",
        "# Run the traning and testing sets in the through the gradient booster\n",
        "\n",
        "rates = np.arange(0.1, 1, 0.05).tolist()\n",
        "estims = np.arange(100, 300, 5).tolist()\n",
        "\n",
        "best_est = estims[0]\n",
        "best_rate = rates[0]\n",
        "\n",
        "for rate in rates:\n",
        "  gb = GradientBoostingClassifier(n_estimators=100, learning_rate=rate)\n",
        "  gb.fit(X_train, y_train)\n",
        "  \n",
        "  print(\"Learning rate: \", rate)\n",
        "  print(\"Accuracy score with training data: {0:.3f}\".format(gb.score(X_train, y_train)))\n",
        "  print(\"Accuracy score with testing data: {0:.3f}\".format(gb.score(X_test, y_test)))\n",
        "  if rate == rates[0]:\n",
        "    my_score = (gb.score(X_train, y_train)+gb.score(X_test, y_test))/2.0\n",
        "    best_rate = rate\n",
        "    continue\n",
        "  if (gb.score(X_train, y_train)+gb.score(X_test, y_test))/2 > my_score:\n",
        "    my_score = (gb.score(X_train, y_train)+gb.score(X_test, y_test))/2.0\n",
        "    best_rate = rate\n",
        "\n",
        "for estim in estims:\n",
        "  gb = GradientBoostingClassifier(n_estimators=estim, learning_rate=best_rate)\n",
        "  gb.fit(X_train, y_train)\n",
        "  \n",
        "  print(\"n_estimators: \", estim)\n",
        "  print(\"Accuracy score with training data: {0:.3f}\".format(gb.score(X_train, y_train)))\n",
        "  print(\"Accuracy score with testing data: {0:.3f}\".format(gb.score(X_test, y_test)))\n",
        "  if estim == estims[0]:\n",
        "    my_score = (gb.score(X_train, y_train)+gb.score(X_test, y_test))/2.0\n",
        "    best_est = estim\n",
        "    continue\n",
        "  if (gb.score(X_train, y_train)+gb.score(X_test, y_test))/2 > my_score:\n",
        "    my_score = (gb.score(X_train, y_train)+gb.score(X_test, y_test))/2.0\n",
        "    best_est = estim\n",
        "\n",
        "print(\"Best learning rate: \", best_rate)\n",
        "print(\"Best n_estimators: \", best_est)\n",
        "print(\"Accuracy score with training data: {0:.3f}\".format(gb.score(X_train, y_train)))\n",
        "print(\"Accuracy score with testing data: {0:.3f}\".format(gb.score(X_test, y_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlZ2OQ0ncaF_",
        "outputId": "380527ee-e642-4597-89bd-a39097d53029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/dataset\n",
            "/content\n",
            "Learning rate:  0.1\n",
            "Accuracy score with training data: 0.888\n",
            "Accuracy score with testing data: 0.885\n",
            "Learning rate:  0.15000000000000002\n",
            "Accuracy score with training data: 0.891\n",
            "Accuracy score with testing data: 0.888\n",
            "Learning rate:  0.20000000000000004\n",
            "Accuracy score with training data: 0.893\n",
            "Accuracy score with testing data: 0.890\n",
            "Learning rate:  0.25000000000000006\n",
            "Accuracy score with training data: 0.894\n",
            "Accuracy score with testing data: 0.891\n",
            "Learning rate:  0.30000000000000004\n",
            "Accuracy score with training data: 0.896\n",
            "Accuracy score with testing data: 0.892\n",
            "Learning rate:  0.3500000000000001\n",
            "Accuracy score with training data: 0.897\n",
            "Accuracy score with testing data: 0.892\n",
            "Learning rate:  0.40000000000000013\n",
            "Accuracy score with training data: 0.898\n",
            "Accuracy score with testing data: 0.894\n",
            "Learning rate:  0.45000000000000007\n",
            "Accuracy score with training data: 0.900\n",
            "Accuracy score with testing data: 0.894\n",
            "Learning rate:  0.5000000000000001\n",
            "Accuracy score with training data: 0.899\n",
            "Accuracy score with testing data: 0.893\n",
            "Learning rate:  0.5500000000000002\n",
            "Accuracy score with training data: 0.900\n",
            "Accuracy score with testing data: 0.896\n",
            "Learning rate:  0.6000000000000002\n",
            "Accuracy score with training data: 0.901\n",
            "Accuracy score with testing data: 0.895\n",
            "Learning rate:  0.6500000000000001\n",
            "Accuracy score with training data: 0.901\n",
            "Accuracy score with testing data: 0.895\n",
            "Learning rate:  0.7000000000000002\n",
            "Accuracy score with training data: 0.901\n",
            "Accuracy score with testing data: 0.896\n",
            "Learning rate:  0.7500000000000002\n",
            "Accuracy score with training data: 0.901\n",
            "Accuracy score with testing data: 0.895\n",
            "Learning rate:  0.8000000000000002\n",
            "Accuracy score with training data: 0.902\n",
            "Accuracy score with testing data: 0.895\n",
            "Learning rate:  0.8500000000000002\n",
            "Accuracy score with training data: 0.902\n",
            "Accuracy score with testing data: 0.896\n",
            "Learning rate:  0.9000000000000002\n",
            "Accuracy score with training data: 0.902\n",
            "Accuracy score with testing data: 0.893\n",
            "Learning rate:  0.9500000000000003\n",
            "Accuracy score with training data: 0.902\n",
            "Accuracy score with testing data: 0.893\n",
            "n_estimators:  100\n",
            "Accuracy score with training data: 0.902\n",
            "Accuracy score with testing data: 0.896\n",
            "n_estimators:  105\n",
            "Accuracy score with training data: 0.903\n",
            "Accuracy score with testing data: 0.896\n",
            "n_estimators:  110\n",
            "Accuracy score with training data: 0.904\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  115\n",
            "Accuracy score with training data: 0.904\n",
            "Accuracy score with testing data: 0.896\n",
            "n_estimators:  120\n",
            "Accuracy score with training data: 0.904\n",
            "Accuracy score with testing data: 0.896\n",
            "n_estimators:  125\n",
            "Accuracy score with training data: 0.905\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  130\n",
            "Accuracy score with training data: 0.905\n",
            "Accuracy score with testing data: 0.896\n",
            "n_estimators:  135\n",
            "Accuracy score with training data: 0.905\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  140\n",
            "Accuracy score with training data: 0.906\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  145\n",
            "Accuracy score with training data: 0.906\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  150\n",
            "Accuracy score with training data: 0.906\n",
            "Accuracy score with testing data: 0.898\n",
            "n_estimators:  155\n",
            "Accuracy score with training data: 0.907\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  160\n",
            "Accuracy score with training data: 0.907\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  165\n",
            "Accuracy score with training data: 0.907\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  170\n",
            "Accuracy score with training data: 0.908\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  175\n",
            "Accuracy score with training data: 0.908\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  180\n",
            "Accuracy score with training data: 0.909\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  185\n",
            "Accuracy score with training data: 0.909\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  190\n",
            "Accuracy score with training data: 0.909\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  195\n",
            "Accuracy score with training data: 0.909\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  200\n",
            "Accuracy score with training data: 0.910\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  205\n",
            "Accuracy score with training data: 0.910\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  210\n",
            "Accuracy score with training data: 0.910\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  215\n",
            "Accuracy score with training data: 0.911\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  220\n",
            "Accuracy score with training data: 0.911\n",
            "Accuracy score with testing data: 0.898\n",
            "n_estimators:  225\n",
            "Accuracy score with training data: 0.911\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  230\n",
            "Accuracy score with training data: 0.911\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  235\n",
            "Accuracy score with training data: 0.912\n",
            "Accuracy score with testing data: 0.897\n",
            "n_estimators:  240\n",
            "Accuracy score with training data: 0.912\n",
            "Accuracy score with testing data: 0.896\n",
            "n_estimators:  245\n",
            "Accuracy score with training data: 0.912\n",
            "Accuracy score with testing data: 0.896\n",
            "n_estimators:  250\n",
            "Accuracy score with training data: 0.912\n",
            "Accuracy score with testing data: 0.896\n",
            "n_estimators:  255\n",
            "Accuracy score with training data: 0.913\n",
            "Accuracy score with testing data: 0.896\n",
            "n_estimators:  260\n",
            "Accuracy score with training data: 0.913\n",
            "Accuracy score with testing data: 0.896\n",
            "n_estimators:  265\n",
            "Accuracy score with training data: 0.913\n",
            "Accuracy score with testing data: 0.896\n",
            "n_estimators:  270\n",
            "Accuracy score with training data: 0.913\n",
            "Accuracy score with testing data: 0.896\n",
            "n_estimators:  275\n",
            "Accuracy score with training data: 0.914\n",
            "Accuracy score with testing data: 0.896\n",
            "n_estimators:  280\n",
            "Accuracy score with training data: 0.914\n",
            "Accuracy score with testing data: 0.896\n",
            "n_estimators:  285\n",
            "Accuracy score with training data: 0.914\n",
            "Accuracy score with testing data: 0.896\n",
            "n_estimators:  290\n",
            "Accuracy score with training data: 0.914\n",
            "Accuracy score with testing data: 0.895\n",
            "n_estimators:  295\n",
            "Accuracy score with training data: 0.914\n",
            "Accuracy score with testing data: 0.895\n",
            "Best learning rate:  0.8500000000000002\n",
            "Best n_estimators:  275\n",
            "Accuracy score with training data: 0.914\n",
            "Accuracy score with testing data: 0.895\n"
          ]
        }
      ]
    }
  ]
}
